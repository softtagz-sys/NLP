{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Large-scale data analysis with spaCy\n",
    "\n",
    "In this chapter, you'll use your new skills to extract specific information from large volumes of text. You'll learn how to make the most of spaCy's data structures, and how to effectively combine statistical and rule-based approaches for text analysis.\n"
   ],
   "id": "bca5e57b7fc327bf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data structures: Vocabulary, Lexemes and StringStore",
   "id": "5c6fc97090ed5752"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T10:22:38.103705Z",
     "start_time": "2024-09-27T10:22:33.589051Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import spacy \n",
    "spacy.cli.download(\"en_core_web_md\")\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "doc = nlp(\"I love coffee\")\n",
    "print(\"hash value:\", nlp.vocab.strings[\"coffee\"])\n",
    "print(\"string value:\", nlp.vocab.strings[3197928453018144401])\n",
    "\n",
    "#The doc also exposes the vocab and strings\n",
    "print(\"hash value:\", doc.vocab.strings[\"coffee\"])"
   ],
   "id": "f416c359b1ffdcfa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[38;5;2m✔ Download and installation successful\u001B[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n",
      "\u001B[38;5;3m⚠ Restart to reload dependencies\u001B[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n",
      "hash value: 3197928453018144401\n",
      "string value: coffee\n",
      "hash value: 3197928453018144401\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-27T10:22:38.128455Z",
     "start_time": "2024-09-27T10:22:38.124884Z"
    }
   },
   "source": [
    "# A lexeme object is an entry in the vocabulary\n",
    "doc = nlp(\"I love coffee\")\n",
    "lexeme = nlp.vocab[\"coffee\"]\n",
    "\n",
    "# Print the lexical attributes\n",
    "print(lexeme.text, lexeme.orth, lexeme.is_alpha)\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coffee 3197928453018144401 True\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Strings to hashes",
   "id": "471a1515ebc7473a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Part 1 \n",
    "\n",
    "\n",
    "    Look up the string “cat” in nlp.vocab.strings to get the hash.\n",
    "    Look up the hash to get back the string.\n",
    "\n",
    " "
   ],
   "id": "63289a3276a4d4f9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T10:22:38.362620Z",
     "start_time": "2024-09-27T10:22:38.359139Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "doc = nlp(\"I have a cat\")\n",
    "\n",
    "# Look up the hash for the word \"cat\"\n",
    "cat_hash = nlp.vocab.strings[\"cat\"]\n",
    "print(cat_hash)\n",
    "\n",
    "# Look up the cat_hash to get the string\n",
    "cat_string = nlp.vocab.strings[cat_hash]\n",
    "print(cat_string)"
   ],
   "id": "f1664f792fefe9ba",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5439657043933447811\n",
      "cat\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Part 2 \n",
    "\n",
    "\n",
    "    Look up the string label “PERSON” in nlp.vocab.strings to get the hash.\n",
    "    Look up the hash to get back the string.\n",
    "\n"
   ],
   "id": "9aa78d04277f7b98"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T10:22:38.376494Z",
     "start_time": "2024-09-27T10:22:38.372459Z"
    }
   },
   "cell_type": "code",
   "source": [
    "doc = nlp(\"David Bowie is a PERSON\")\n",
    "\n",
    "# Look up the hash for the string label \"PERSON\"\n",
    "person_hash = nlp.vocab.strings[\"PERSON\"]\n",
    "print(person_hash)\n",
    "\n",
    "# Look up the person_hash to get the string\n",
    "person_string = nlp.vocab.strings[person_hash]\n",
    "print(person_string)"
   ],
   "id": "45d7e6c7cc0df868",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "380\n",
      "PERSON\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data structures 2: Doc, Span and Token",
   "id": "153685153cbf6cff"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## The Doc object",
   "id": "a4b2f0885213a2cf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T10:22:38.413756Z",
     "start_time": "2024-09-27T10:22:38.407543Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "# The words and spaces to create the doc from\n",
    "words = [\"Hello\", \"world\", \"!\"]\n",
    "spaces = [True, False, False]\n",
    "\n",
    "# Create a doc manually\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "doc"
   ],
   "id": "aa24a23d3677d507",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hello world!"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## The Span object",
   "id": "473471e795c7928b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T10:22:38.438915Z",
     "start_time": "2024-09-27T10:22:38.434445Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# The words and spaces to create the doc from\n",
    "words = [\"Hello\", \"world\", \"!\"]\n",
    "spaces = [True, False, False]\n",
    "\n",
    "# Create a doc manually\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "\n",
    "# Create a span manually\n",
    "span = Span(doc, 0, 2)\n",
    "\n",
    "# Create a span with a label\n",
    "span_with_label = Span(doc, 0, 2, label=\"GREETING\")\n",
    "\n",
    "# Add span to the doc.ents\n",
    "doc.ents = [span_with_label]\n",
    "span "
   ],
   "id": "426c4b3489bc046f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hello world"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Creating a Doc\n",
    "Let’s create some Doc objects from scratch!"
   ],
   "id": "694be0acc5fea8f1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Part 1 \n",
    "\n",
    "\n",
    "    Import the Doc from spacy.tokens.\n",
    "    Create a Doc from the words and spaces."
   ],
   "id": "f9904e1b5965ff3e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T10:22:38.462462Z",
     "start_time": "2024-09-27T10:22:38.458147Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Desired text: \"spaCy is cool!\"\n",
    "words = [\"spaCy\", \"is\", \"cool\", \"!\"]\n",
    "spaces = [True, True, False, False]\n",
    "\n",
    "# Create a Doc from the words and spaces\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)"
   ],
   "id": "c3f5b8b211d195d4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy is cool!\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Part 2 \n",
    "\n",
    "\n",
    "    Import the Doc from spacy.tokens.\n",
    "    Complete the words and spaces. Don't forget to pass in the vocab!"
   ],
   "id": "51a4c58dbe1ae30a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T10:22:38.493998Z",
     "start_time": "2024-09-27T10:22:38.490590Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Desired text: \"Go, get started!\"\n",
    "words = [\"Go\", \",\", \"get\", \"started\", \"!\"]\n",
    "spaces = [False, True, True, False, False]\n",
    "\n",
    "# Create a Doc from the words and spaces\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)"
   ],
   "id": "c9afb462e441af1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go, get started!\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Part 3\n",
    "\n",
    "\n",
    "    Import the Doc from spacy.tokens.\n",
    "    Complete the words and spaces to match the desired text and create a doc.\n"
   ],
   "id": "36c2bcc983c9040b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T10:22:38.514637Z",
     "start_time": "2024-09-27T10:22:38.511315Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Desired text: \"Oh, really?!\"\n",
    "words = [\"Oh\", \",\", \"really\", \"?\", \"!\"]\n",
    "spaces = [False, True, False, False, False]\n",
    "\n",
    "# Create a Doc from the words and spaces\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)"
   ],
   "id": "fa2909e584a89563",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, really?!\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Docs, spans and entities from scratch\n",
    "In this exercise, you’ll create the Doc and Span objects manually, and update the named entities – just like spaCy does behind the scenes. A shared nlp object has already been created.\n",
    "\n",
    "    Import the Doc and Span classes from spacy.tokens.\n",
    "    Use the Doc class directly to create a doc from the words and spaces.\n",
    "    Create a Span for “David Bowie” from the doc and assign it the label \"PERSON\".\n",
    "    Overwrite the doc.ents with a list of one entity, the “David Bowie” span.\n"
   ],
   "id": "d632a070c735ca54"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T10:22:38.537473Z",
     "start_time": "2024-09-27T10:22:38.533434Z"
    }
   },
   "cell_type": "code",
   "source": [
    "words = [\"I\", \"like\", \"David\", \"Bowie\"]\n",
    "spaces = [True, True, True, False]\n",
    "\n",
    "# Create a doc from the words and spaces\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)\n",
    "\n",
    "# Create a span for \"David Bowie\" from the doc and assign it the label \"PERSON\"\n",
    "span = Span(doc, 2, 4, label=\"PERSON\")\n",
    "print(span.text, span.label_)\n",
    "\n",
    "# Add the span to the doc's entities\n",
    "doc.ents = [span]\n",
    "\n",
    "# Print entities' text and labels\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ],
   "id": "a998a1704c271641",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like David Bowie\n",
      "David Bowie PERSON\n",
      "[('David Bowie', 'PERSON')]\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data structures best practices",
   "id": "f5294e8140194f9a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T10:22:38.563876Z",
     "start_time": "2024-09-27T10:22:38.560105Z"
    }
   },
   "cell_type": "code",
   "source": [
    "doc = nlp(\"Berlin looks like a nice city\")\n",
    "\n",
    "# Iterate over the tokens\n",
    "for token in doc:\n",
    "    # Check if the current token is a proper noun\n",
    "    if token.pos_ == \"PROPN\":\n",
    "        # Check if the next token is a verb\n",
    "        if doc[token.i + 1].pos_ == \"VERB\":\n",
    "            print(\"Found proper noun before a verb:\", token.text)\n",
    "        "
   ],
   "id": "222af95ce33a0808",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Word vectors and semantic similarity",
   "id": "11314c9d1022df71"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T10:22:39.489961Z",
     "start_time": "2024-09-27T10:22:38.569290Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load a larger model with vectors\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Compare two documents\n",
    "doc1 = nlp(\"I like fast food\")\n",
    "doc2 = nlp(\"I like pizza\")\n",
    "print(doc1.similarity(doc2))"
   ],
   "id": "de0c86e55064468f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8698332283318978\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T10:23:54.001762Z",
     "start_time": "2024-09-27T10:23:53.993123Z"
    }
   },
   "cell_type": "code",
   "source": [
    "doc = nlp(\"I like pizza and pasta\")\n",
    "token1 = doc[2]\n",
    "token2 = doc[4]\n",
    "print(token1.similarity(token2))"
   ],
   "id": "2ddb52e29c6cdeb6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.685019850730896\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T10:24:05.296928Z",
     "start_time": "2024-09-27T10:24:05.282512Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Compare a document with a token\n",
    "doc = nlp(\"I like pizza\")\n",
    "token = nlp(\"soap\")[0]\n",
    "\n",
    "print(doc.similarity(token))"
   ],
   "id": "40d257b65a5e1e17",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1821369691957915\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T10:24:04.266916Z",
     "start_time": "2024-09-27T10:24:04.251108Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Compare a span with a document\n",
    "span = nlp(\"I like pizza and pasta\")[2:5]\n",
    "doc = nlp(\"McDonalds sells burgers\")\n",
    "\n",
    "print(span.similarity(doc))"
   ],
   "id": "7dd3ee933d168166",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47190033157126826\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T10:24:18.817303Z",
     "start_time": "2024-09-27T10:24:18.803932Z"
    }
   },
   "cell_type": "code",
   "source": [
    "doc = nlp(\"I have a banana\")\n",
    "# Access the vector via the token.vector attribute\n",
    "print(doc[3].vector)"
   ],
   "id": "77226f4d1592f67b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.20778  -2.4151    0.36605   2.0139   -0.23752  -3.1952   -0.2952\n",
      "  1.2272   -3.4129   -0.54969   0.32634  -1.0813    0.55626   1.5195\n",
      "  0.97797  -3.1816   -0.37207  -0.86093   2.1509   -4.0845    0.035405\n",
      "  3.5702   -0.79413  -1.7025   -1.6371   -3.198    -1.9387    0.91166\n",
      "  0.85409   1.8039   -1.103    -2.5274    1.6365   -0.82082   1.0278\n",
      " -1.705     1.5511   -0.95633  -1.4702   -1.865    -0.19324  -0.49123\n",
      "  2.2361    2.2119    3.6654    1.7943   -0.20601   1.5483   -1.3964\n",
      " -0.50819   2.1288   -2.332     1.3539   -2.1917    1.8923    0.28472\n",
      "  0.54285   1.2309    0.26027   1.9542    1.1739   -0.40348   3.2028\n",
      "  0.75381  -2.7179   -1.3587   -1.1965   -2.0923    2.2855   -0.3058\n",
      " -0.63174   0.70083   0.16899   1.2325    0.97006  -0.23356  -2.094\n",
      " -1.737     3.6075   -1.511    -0.9135    0.53878   0.49268   0.44751\n",
      "  0.6315    1.4963    4.1725    2.1961   -1.2409    0.4214    2.9678\n",
      "  1.841     3.0133   -4.4652    0.96521  -0.29787   4.3386   -1.2527\n",
      " -1.7734   -3.5637   -0.20035  -3.3013    0.99951  -0.92888  -0.94594\n",
      "  1.5124   -3.9385    2.7935   -3.1042    3.3382    0.54513  -0.37663\n",
      "  2.5151    0.51468  -0.88907   1.011     3.4705   -3.6037    1.3702\n",
      "  2.3468    1.6674    1.3904   -2.8112    2.237    -1.0344   -0.57164\n",
      "  1.0641   -1.6919    1.958    -0.78305   0.14741   0.51083   1.8278\n",
      " -0.69638   0.90548   0.62282  -1.8315   -2.8587    0.48424  -2.0527\n",
      " -0.53808  -2.3472    1.0354   -1.8257   -0.3892   -0.24943   0.8651\n",
      " -1.5195    1.2166   -2.698    -0.96698   2.2175   -0.16089  -0.49677\n",
      " -0.19646   1.3284    4.0824    1.3919    0.80669  -1.0316   -0.28056\n",
      " -1.8632    0.47716  -0.53628   1.3853   -2.1755   -0.2354    2.4933\n",
      " -0.87255   1.4493   -0.10778  -0.44159   1.3462    4.4211   -1.8385\n",
      "  0.3985    0.47637  -0.60074   3.3583   -0.15006  -0.40495   2.7225\n",
      " -1.6297    0.86797  -4.1445   -2.7793    1.1535   -0.011691  0.9792\n",
      " -1.0141    0.80134   0.43642   1.4337    2.8927    0.82871  -1.1827\n",
      " -1.3838    2.3903   -0.89323   1.1461   -1.7435    0.8654   -0.27075\n",
      " -0.78698   1.5631   -0.5923    0.098082 -0.26682   1.6282   -0.77495\n",
      "  3.2552    1.7964   -1.4314    1.2336    2.3102   -1.6328    2.8366\n",
      " -0.71384   0.43967   1.5627    3.079    -0.922    -0.43981  -0.7659\n",
      "  1.9362   -2.2479    1.041     0.63206   1.5855    3.4097   -2.9204\n",
      " -1.4751   -0.59534  -1.688    -4.1362    2.745    -2.8515    3.6509\n",
      " -0.66993  -2.8794    2.0733    1.1779   -2.0307    2.595    -0.12246\n",
      "  1.5844    1.1855    0.022385 -2.2916   -2.2684   -2.7537    0.34981\n",
      " -4.6243   -0.96521  -1.1435   -2.8894   -0.12619   2.9577   -1.7227\n",
      "  0.24757   1.2149    3.5349   -0.95802   0.080346 -1.6553   -0.6734\n",
      "  2.2918   -1.8229   -1.1336    1.8884    2.4789   -0.66061   2.0529\n",
      " -0.76687   0.32362  -2.2579    0.91278   0.36231   0.61562  -0.15396\n",
      " -0.42917  -0.89848   0.17298  -0.76978  -2.0222   -1.7127   -1.5632\n",
      "  0.56631  -1.354     2.6261    1.9156   -1.5651    1.8315   -1.4257\n",
      " -1.6861   -0.51953   1.7635   -0.50722   1.388    -1.1012  ]\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T10:24:34.328274Z",
     "start_time": "2024-09-27T10:24:34.315448Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Similarity depends on the application context\n",
    "doc1 = nlp(\"I like cats\")\n",
    "doc2 = nlp(\"I hate cats\")\n",
    "\n",
    "print(doc1.similarity(doc2))"
   ],
   "id": "68f5d9f78d2317dd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9530094042245597\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Inspecting word vectors\n",
   "id": "6af1b375274cbeae"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T10:25:23.790663Z",
     "start_time": "2024-09-27T10:25:23.780541Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Process a text\n",
    "doc = nlp(\"Two bananas in pyjamas\")\n",
    "\n",
    "# Get the vector for the token \"bananas\"\n",
    "bananas_vector = doc[1].vector\n",
    "print(bananas_vector)"
   ],
   "id": "6374b85803e97a7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.1689e-01 -2.5989e+00 -1.3144e+00  2.2500e+00 -4.6767e-01 -2.0695e+00\n",
      " -6.3379e-01 -4.0222e-01 -3.4022e+00 -3.6932e-01 -7.9938e-01 -1.0412e+00\n",
      "  9.3756e-01  1.6070e+00  8.8330e-01 -2.8483e+00  1.3349e-01 -3.1656e+00\n",
      "  8.1896e-01 -4.8113e+00  1.5655e+00  1.6665e+00 -4.7081e-01 -1.9475e+00\n",
      " -1.1779e+00 -1.3810e+00 -2.0071e+00 -2.1639e-01  9.0609e-01  1.5279e+00\n",
      "  1.2587e-04 -2.9000e+00  7.6069e-01 -2.2825e+00  1.2495e-02 -1.5653e+00\n",
      "  2.0052e+00 -1.7747e+00  5.9220e-01 -1.1428e+00 -1.3441e+00  3.4784e-01\n",
      "  1.7492e+00  1.9086e+00  1.0600e+00  1.2965e+00  4.1431e-01  7.9416e-01\n",
      " -1.1277e+00 -1.1403e+00  7.5891e-01 -9.4419e-01  1.4413e+00 -2.2554e+00\n",
      "  1.6226e-01  3.8901e-01  1.2299e-01  1.1577e+00  1.5524e+00  1.3853e+00\n",
      "  1.1112e+00  7.5767e-01  3.9431e+00 -2.8506e-01 -2.1645e+00 -1.0862e+00\n",
      " -1.4973e+00 -1.2781e+00  2.4643e+00 -1.5886e+00  2.5679e-01  6.4918e-01\n",
      "  1.6809e-01  5.7693e-01  3.1121e-01 -4.5278e-01 -2.7555e+00 -2.1846e+00\n",
      "  4.4865e+00  2.7107e-01 -5.3831e-01  8.3013e-01  6.7752e-02  1.4234e-01\n",
      "  1.2585e+00 -8.5423e-01  9.2971e-01 -3.9940e-01 -5.8663e-01  6.6604e-01\n",
      "  2.3871e+00  4.9333e-01  2.3922e+00 -3.7396e+00 -3.9524e-01 -6.3799e-01\n",
      "  3.3500e+00 -2.0430e+00 -1.5601e+00 -2.3594e+00 -1.4671e+00 -3.2848e+00\n",
      "  1.5197e+00 -1.1674e+00 -1.2885e+00  3.4890e+00 -4.0526e+00  1.6946e+00\n",
      " -1.5310e+00  2.6790e+00  1.2865e+00 -2.9286e-02  3.1037e+00  1.3635e-01\n",
      " -1.3327e-01  5.4603e-01  1.2937e+00 -2.3662e+00  2.8862e-01  1.6226e+00\n",
      "  6.3531e-01  1.5498e+00 -2.3349e+00  1.6150e+00 -2.0071e+00 -4.8784e-01\n",
      "  1.7768e+00 -2.6920e+00  1.6341e+00  4.0537e-01  3.7324e-01  7.8494e-01\n",
      "  1.3917e+00 -2.9035e-01  2.5224e+00  1.2485e+00 -1.5159e+00 -2.0832e+00\n",
      " -1.8766e-01 -1.3394e+00 -5.3597e-01 -2.4915e+00  1.6341e+00 -3.0336e+00\n",
      "  1.8791e-01 -2.4776e-01  1.6347e+00 -7.0009e-01  2.1221e+00 -2.3470e+00\n",
      " -2.1513e+00  1.2630e+00 -2.8195e-01 -5.6535e-01 -9.0373e-01  1.3455e+00\n",
      "  4.1099e+00  6.9325e-01  4.6835e-01  2.9949e-01 -3.2456e-01 -2.1713e+00\n",
      "  4.6691e-01 -1.3795e-01  6.0910e-01 -1.3374e+00 -8.3586e-01  1.8260e+00\n",
      " -4.5386e-01  1.2555e+00 -6.6705e-01 -3.0835e-01 -1.4692e-01  3.9952e+00\n",
      " -1.1289e+00  1.7926e-01 -5.8095e-01  6.2500e-01  2.6151e+00 -1.3212e+00\n",
      " -1.9355e+00  2.4898e+00 -1.7301e+00  8.6154e-01 -2.5272e+00 -3.0166e+00\n",
      "  5.8867e-01 -1.4207e-01  1.7611e+00 -1.0399e+00  5.7063e-01  7.2554e-01\n",
      "  8.7684e-01  1.6996e+00  1.1084e+00 -2.6877e-01 -2.2970e+00  1.8850e+00\n",
      "  1.2536e-01  1.1671e+00 -1.3038e+00  8.2640e-01  3.8002e-01 -1.0354e+00\n",
      "  2.2547e+00 -2.4526e-01  8.3622e-01 -7.9525e-01  1.4251e+00  4.5795e-02\n",
      "  2.5062e+00  7.2175e-01 -3.8197e-01  1.1387e+00  1.6779e+00 -1.6925e+00\n",
      "  8.2290e-02 -3.3117e-01 -6.1974e-01  8.0306e-01  2.1189e+00 -7.8231e-01\n",
      "  3.3266e-01 -7.1646e-01  1.7721e+00 -1.2573e+00  1.5601e-01  9.6129e-01\n",
      "  4.8986e-01  3.1657e+00 -3.0332e+00 -9.1339e-01 -2.8627e-01 -9.8174e-01\n",
      " -2.7599e+00  2.9160e+00 -1.1540e+00  2.9537e+00 -1.1502e+00 -2.2760e+00\n",
      "  1.1679e+00  2.0314e-01 -2.2882e+00  1.0485e+00 -4.9545e-01  2.4065e+00\n",
      "  9.1433e-01  1.3808e-01 -1.1361e+00 -1.2278e+00 -1.8958e+00 -2.4934e-01\n",
      " -4.1792e+00 -1.3781e+00 -8.9829e-01 -1.4802e+00 -1.2894e+00  2.2336e+00\n",
      " -2.4411e+00  3.0410e-01  2.1452e+00  1.8540e+00 -3.3640e-01  4.2768e-01\n",
      " -4.2577e-01  2.2337e-01  3.3650e+00 -2.1996e-01 -1.7426e+00  1.4554e+00\n",
      "  2.5087e+00 -1.1257e+00  1.4096e+00  1.3704e-01  7.6963e-01 -7.6688e-02\n",
      " -6.1875e-01 -2.0445e-01  4.9965e-01 -6.7246e-01 -4.0058e-01 -1.1536e+00\n",
      " -9.2837e-01 -2.5444e+00 -1.9880e-01 -2.1432e+00 -1.8218e+00 -5.7831e-01\n",
      " -1.6785e+00  2.6610e+00  1.0343e+00 -1.4297e+00  1.4563e+00 -2.0388e+00\n",
      " -1.4377e+00 -9.7985e-01  2.9488e+00 -2.7107e-01 -2.4010e-01 -1.3845e-01]\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Comparing similarities\n",
    "In this exercise, you’ll be using spaCy’s similarity methods to compare Doc, Token and Span objects and get similarity scores."
   ],
   "id": "cd9934f5fda23eaa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Part 1 \n",
    "\n",
    "    Use the doc.similarity method to compare doc1 to doc2 and print the result.\n",
    "\n"
   ],
   "id": "e8f1dfca38496d43"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T10:26:03.644249Z",
     "start_time": "2024-09-27T10:26:03.631839Z"
    }
   },
   "cell_type": "code",
   "source": [
    "doc1 = nlp(\"It's a warm summer day\")\n",
    "doc2 = nlp(\"It's sunny outside\")\n",
    "\n",
    "# Get the similarity of doc1 and doc2\n",
    "similarity = doc1.similarity(doc2)\n",
    "print(similarity)"
   ],
   "id": "1e920ec3fba29b5e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8220092482601077\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Part 2 \n",
    "\n",
    "    Use the token.similarity method to compare token1 to token2 and print the result.\n"
   ],
   "id": "a9a8d94f41fb206"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T10:26:26.797858Z",
     "start_time": "2024-09-27T10:26:26.788881Z"
    }
   },
   "cell_type": "code",
   "source": [
    "doc = nlp(\"TV and books\")\n",
    "token1, token2 = doc[0], doc[2]\n",
    "\n",
    "# Get the similarity of the tokens \"TV\" and \"books\"\n",
    "similarity = token1.similarity(token2)\n",
    "print(similarity)"
   ],
   "id": "5e3128c8b2101f7b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10219937562942505\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Part 3\n",
    "\n",
    "    Create spans for “great restaurant”/“really nice bar”.\n",
    "    Use span.similarity to compare them and print the result.\n",
    "\n"
   ],
   "id": "5fa0edeb4e52609d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T10:26:49.220475Z",
     "start_time": "2024-09-27T10:26:49.210373Z"
    }
   },
   "cell_type": "code",
   "source": [
    "doc = nlp(\"This was a great restaurant. Afterwards, we went to a really nice bar.\")\n",
    "\n",
    "# Create spans for \"great restaurant\" and \"really nice bar\"\n",
    "span1 = doc[3:5]\n",
    "span2 = doc[12:15]\n",
    "\n",
    "# Get the similarity of the spans\n",
    "similarity = span1.similarity(span2)\n",
    "print(similarity)"
   ],
   "id": "e3a9a49041f6411d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6348510384559631\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Combining Predictions and Rules ",
   "id": "2e96fe6e6591f555"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T10:33:48.795159Z",
     "start_time": "2024-09-27T10:33:48.784408Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Patterns are lists of dictionaries describing the tokens\n",
    "pattern = [{\"LEMMA\": \"love\", \"POS\": \"VERB\"}, {\"LOWER\": \"cats\"}]\n",
    "matcher.add(\"LOVE_CATS\", [pattern])\n",
    "\n",
    "# Operators can specify how often a token should be matched\n",
    "pattern = [{\"TEXT\": \"very\", \"OP\": \"+\"}, {\"TEXT\": \"happy\"}]\n",
    "matcher.add(\"VERY_HAPPY\", [pattern])\n",
    "\n",
    "# Calling matcher on doc returns list of (match_id, start, end) tuples\n",
    "doc = nlp(\"I love cats and I'm very very happy\")\n",
    "matches = matcher(doc)\n",
    "matches"
   ],
   "id": "b856d0d74a47a189",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9137535031263442622, 1, 3),\n",
       " (2447047934687575526, 7, 9),\n",
       " (2447047934687575526, 6, 9)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Adding statistical predictions",
   "id": "3741628b323d3ba0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T10:34:02.622140Z",
     "start_time": "2024-09-27T10:34:02.611753Z"
    }
   },
   "cell_type": "code",
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"DOG\", [[{\"LOWER\": \"golden\"}, {\"LOWER\": \"retriever\"}]])\n",
    "doc = nlp(\"I have a Golden Retriever\")\n",
    "\n",
    "for match_id, start, end in matcher(doc):\n",
    "    span = doc[start:end]\n",
    "    print(\"Matched span:\", span.text)\n",
    "    # Get the span's root token and root head token\n",
    "    print(\"Root token:\", span.root.text)\n",
    "    print(\"Root head token:\", span.root.head.text)\n",
    "    # Get the previous token and its POS tag\n",
    "    print(\"Previous token:\", doc[start - 1].text, doc[start - 1].pos_)"
   ],
   "id": "8c6401bf498b6c67",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched span: Golden Retriever\n",
      "Root token: Retriever\n",
      "Root head token: have\n",
      "Previous token: a DET\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Efficient phrase matching",
   "id": "64ccb670ad9c496f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T10:34:39.734501Z",
     "start_time": "2024-09-27T10:34:39.718940Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "pattern = nlp(\"Golden Retriever\")\n",
    "matcher.add(\"DOG\", [pattern])\n",
    "doc = nlp(\"I have a Golden Retriever\")\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Get the matched span\n",
    "    span = doc[start:end]\n",
    "    print(\"Matched span:\", span.text)"
   ],
   "id": "533fefd9bbb5ac88",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched span: Golden Retriever\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Debugging patterns\n",
    "Both patterns in this exercise contain mistakes and won’t match as expected. Can you fix them? If you get stuck, try printing the tokens in the doc to see how the text will be split and adjust the pattern so that each dictionary represents one token.\n",
    "\n",
    "    Edit pattern1 so that it correctly matches all case-insensitive mentions of \"Amazon\" plus a title-cased proper noun.\n",
    "    Edit pattern2 so that it correctly matches all case-insensitive mentions of \"ad-free\", plus the following noun"
   ],
   "id": "db7a95eb81e996fc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T10:38:40.781368Z",
     "start_time": "2024-09-27T10:38:40.425580Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\n",
    "    \"Twitch Prime, the perks program for Amazon Prime members offering free \"\n",
    "    \"loot, games and other benefits, is ditching one of its best features: \"\n",
    "    \"ad-free viewing. According to an email sent out to Amazon Prime members \"\n",
    "    \"today, ad-free viewing will no longer be included as a part of Twitch \"\n",
    "    \"Prime for new members, beginning on September 14. However, members with \"\n",
    "    \"existing annual subscriptions will be able to continue to enjoy ad-free \"\n",
    "    \"viewing until their subscription comes up for renewal. Those with \"\n",
    "    \"monthly subscriptions will have access to ad-free viewing until October 15.\"\n",
    ")\n",
    "\n",
    "# Create the match patterns\n",
    "pattern1 = [{\"LOWER\": \"amazon\"}, {\"IS_TITLE\": True, \"POS\": \"PROPN\"}]\n",
    "pattern2 = [{\"LOWER\": \"ad\"}, {\"TEXT\": \"-\"}, {\"LOWER\": \"free\"}, {\"POS\": \"NOUN\"}]\n",
    "\n",
    "# Initialize the Matcher and add the patterns\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"PATTERN1\", [pattern1])\n",
    "matcher.add(\"PATTERN2\", [pattern2])\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Print pattern string name and text of matched span\n",
    "    print(doc.vocab.strings[match_id], doc[start:end].text)"
   ],
   "id": "981efaa9294ccfaf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATTERN1 Amazon Prime\n",
      "PATTERN2 ad-free viewing\n",
      "PATTERN1 Amazon Prime\n",
      "PATTERN2 ad-free viewing\n",
      "PATTERN2 ad-free viewing\n",
      "PATTERN2 ad-free viewing\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Efficient phrase matching\n",
    "\n",
    "Sometimes it’s more efficient to match exact strings instead of writing patterns describing the individual tokens. This is especially true for finite categories of things – like all countries of the world. We already have a list of countries, so let’s use this as the basis of our information extraction script. A list of string names is available as the variable COUNTRIES.\n",
    "\n",
    "    Import the PhraseMatcher and initialize it with the shared vocab as the variable matcher.\n",
    "    Add the phrase patterns and call the matcher on the doc.\n",
    "    \n",
    "**Look on site, they use a json they don't give out. , same with the next task. **\n",
    "\n"
   ],
   "id": "32f75c68964dc7c6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "981481aaa037e2b1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
